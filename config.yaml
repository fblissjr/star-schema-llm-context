# LLM Memory System Configuration
# Customize these settings for your environment

database:
  # DuckDB connection settings
  path: "data/llm_memory.duckdb"
  threads: 4
  memory_limit: "4GB"
  
  # Performance tuning
  enable_optimizer: true
  parallel_execution: true
  
context:
  # Maximum tokens for context window
  max_tokens: 100000
  
  # Context prioritization weights (0-1.0)
  # Higher weights = higher priority
  prioritization:
    active_tasks: 0.95        # Current work in progress
    recent_interactions: 0.85  # Recent LLM interactions
    critical_files: 0.75      # Files marked as critical
    hot_spots: 0.70           # Frequently modified areas
    code_understanding: 0.60  # Analyzed code elements
    error_patterns: 0.55      # Known issues
    dependencies: 0.50        # File dependencies
    concepts: 0.40           # High-level patterns
  
  # Context compression
  compression:
    enabled: true
    method: "relevance"  # relevance | summary | hybrid
    threshold: 0.8      # Compress when > 80% of max tokens
  
  # Staleness management
  staleness:
    decay_rate: 0.05    # 5% relevance decay per day
    minimum_score: 0.1  # Don't drop below this relevance
    refresh_interval: 86400  # Seconds (24 hours)

graph:
  # Graph traversal settings
  max_traversal_depth: 10
  max_path_length: 20
  
  # Performance thresholds
  sampling_threshold: 10000  # Sample when graph > this size
  sampling_size: 1000       # Number of nodes to sample
  
  # Community detection
  community_resolution: 1.0  # Higher = smaller communities
  min_community_size: 3
  
  # Pattern detection
  patterns:
    circular_dependencies: true
    god_classes: true
    dead_code: true
    code_clones: true
    long_methods: true
  
  # Thresholds for anti-patterns
  thresholds:
    god_class_methods: 20
    god_class_dependencies: 15
    long_method_lines: 50
    clone_similarity: 0.8

algorithms:
  # PageRank settings
  pagerank:
    damping_factor: 0.85
    iterations: 20
    convergence_threshold: 0.0001
  
  # Centrality computations
  centrality:
    compute_degree: true
    compute_betweenness: true
    compute_closeness: true
    sample_size: 1000  # For large graphs
  
  # Clustering coefficient
  clustering:
    enabled: true
    sample_size: 100  # Sample for performance

analysis:
  # Code analysis settings
  languages:
    - python
    - javascript
    - typescript
    # Add more as needed
  
  # File types to analyze
  file_types:
    source:
      - ".py"
      - ".js"
      - ".ts"
      - ".java"
      - ".cpp"
      - ".c"
      - ".rs"
      - ".go"
    config:
      - ".json"
      - ".yaml"
      - ".yml"
      - ".toml"
    test:
      - "_test.py"
      - ".test.js"
      - ".spec.ts"
    documentation:
      - ".md"
      - ".rst"
      - ".txt"
  
  # Parsing settings
  parsing:
    max_file_size_mb: 10
    skip_binary: true
    follow_symlinks: false
    ignore_patterns:
      - "__pycache__"
      - "node_modules"
      - ".git"
      - ".venv"
      - "dist"
      - "build"

storage:
  # Materialized views
  materialized_views:
    enabled: true
    refresh_interval: 3600  # Seconds (1 hour)
    views:
      - mv_file_dependencies
      - mv_call_graph
      - mv_hot_spots
  
  # Caching
  cache:
    enabled: true
    max_size_mb: 100
    ttl_seconds: 3600
    
  # Backup settings
  backup:
    enabled: false
    interval: 86400  # Daily
    location: "./backups"
    retention_days: 30

monitoring:
  # Performance monitoring
  metrics:
    enabled: true
    log_slow_queries: true
    slow_query_threshold_ms: 1000
  
  # Logging
  logging:
    level: "INFO"  # DEBUG | INFO | WARNING | ERROR
    file: "llm_memory.log"
    max_size_mb: 50
    backup_count: 5
  
  # Health checks
  health:
    check_interval: 60  # Seconds
    checks:
      - database_connection
      - memory_usage
      - disk_space

# Environment-specific overrides
environments:
  development:
    database:
      memory_limit: "2GB"
    context:
      max_tokens: 50000
    monitoring:
      logging:
        level: "DEBUG"
  
  production:
    database:
      memory_limit: "16GB"
      threads: 8
    context:
      max_tokens: 200000
    storage:
      backup:
        enabled: true
    monitoring:
      metrics:
        enabled: true